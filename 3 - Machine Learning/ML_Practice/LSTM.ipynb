{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d19252-6afc-4eb3-b90b-b64bd5c008f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prophet\n",
      "  Downloading prophet-1.1.6-py3-none-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting cmdstanpy>=1.0.4 (from prophet)\n",
      "  Downloading cmdstanpy-1.2.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from prophet) (1.24.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from prophet) (3.10.1)\n",
      "Requirement already satisfied: pandas>=1.0.4 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from prophet) (2.2.3)\n",
      "Collecting holidays<1,>=0.25 (from prophet)\n",
      "  Downloading holidays-0.71-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tqdm>=4.36.1 (from prophet)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting importlib-resources (from prophet)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting stanio<2.0.0,>=0.4.0 (from cmdstanpy>=1.0.4->prophet)\n",
      "  Downloading stanio-0.5.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from matplotlib>=2.0.0->prophet) (3.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from pandas>=1.0.4->prophet) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from tqdm>=4.36.1->prophet) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.16.0)\n",
      "Downloading prophet-1.1.6-py3-none-win_amd64.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.9/13.3 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 6.0/13.3 MB 15.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.7/13.3 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 14.7 MB/s eta 0:00:00\n",
      "Downloading cmdstanpy-1.2.5-py3-none-any.whl (94 kB)\n",
      "Downloading holidays-0.71-py3-none-any.whl (917 kB)\n",
      "   ---------------------------------------- 0.0/917.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 917.9/917.9 kB 21.2 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading stanio-0.5.1-py3-none-any.whl (8.1 kB)\n",
      "Installing collected packages: tqdm, stanio, importlib-resources, holidays, cmdstanpy, prophet\n",
      "Successfully installed cmdstanpy-1.2.5 holidays-0.71 importlib-resources-6.5.2 prophet-1.1.6 stanio-0.5.1 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf41bd01-3a23-4261-9049-35bbeb271ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas_market_calendars\n",
      "  Downloading pandas_market_calendars-5.1.0-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pandas>=1.1 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from pandas_market_calendars) (2.2.3)\n",
      "Requirement already satisfied: tzdata in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from pandas_market_calendars) (2025.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
      "Collecting exchange-calendars>=3.3 (from pandas_market_calendars)\n",
      "  Downloading exchange_calendars-4.10-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.24.0)\n",
      "Collecting pyluach (from exchange-calendars>=3.3->pandas_market_calendars)\n",
      "  Downloading pyluach-2.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting toolz (from exchange-calendars>=3.3->pandas_market_calendars)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting korean_lunar_calendar (from exchange-calendars>=3.3->pandas_market_calendars)\n",
      "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\keert\\anaconda3\\envs\\jupyternotebook-intwebappenv\\lib\\site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n",
      "Downloading pandas_market_calendars-5.1.0-py3-none-any.whl (123 kB)\n",
      "Downloading exchange_calendars-4.10-py3-none-any.whl (198 kB)\n",
      "Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
      "Downloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Installing collected packages: korean_lunar_calendar, toolz, pyluach, exchange-calendars, pandas_market_calendars\n",
      "Successfully installed exchange-calendars-4.10 korean_lunar_calendar-0.3.1 pandas_market_calendars-5.1.0 pyluach-2.2.0 toolz-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas_market_calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b4c31c1-3b27-4bf4-b427-38b11200f944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Resampled data to business days. Shape before NaN handling: (522, 17)\n",
      "Shape after NaN handling: (522, 17)\n",
      "Preprocessing complete.\n",
      "Scaling data...\n",
      "Data scaling complete.\n",
      "\n",
      "--- Stage 1: Predicting Future Exogenous Variables (Multivariate LSTM) ---\n",
      "Exogenous data shapes: X=(512, 10, 16), y=(512, 16)\n",
      "Training Stage 1 LSTM for 50 epochs...\n",
      "Stage 1 Training complete.\n",
      "Generating 15 steps of future exogenous predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d934bf29cc584398ab86e72f592c4f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exog Forecast Steps:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse transforming exogenous predictions...\n",
      "Warning: Could only generate 14 business days. Adjusting N_FORECAST_BDAYS.\n",
      "Future exogenous predictions generated and inverse transformed.\n",
      "\n",
      "--- Stage 2: Predicting Future MF_NAV (LSTM with Exog) ---\n",
      "Target data shapes after structuring: X=(512, 10, 17), y=(512, 1)\n",
      "Training Stage 2 LSTM for 50 epochs...\n",
      "Stage 2 Training complete.\n",
      "Generating 14 steps of future MF_NAV predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51492c3ee6954fcc803bb12aaed884e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Target Forecast Steps:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse transforming target (MF_NAV) predictions...\n",
      "\n",
      "--- FINAL Predicted MF_NAV using Multivariate LSTM Approach ---\n",
      "--- Forecast for Business Days: 2025-01-02 to 2025-01-23 ---\n",
      "            Predicted_MF_NAV\n",
      "2025-01-02           14.0001\n",
      "2025-01-03           13.9694\n",
      "2025-01-06           13.9377\n",
      "2025-01-07           13.9132\n",
      "2025-01-08           13.8836\n",
      "2025-01-10           13.8610\n",
      "2025-01-13           13.8441\n",
      "2025-01-14           13.8335\n",
      "2025-01-15           13.8187\n",
      "2025-01-16           13.8088\n",
      "2025-01-17           13.8005\n",
      "2025-01-21           13.7993\n",
      "2025-01-22           13.7985\n",
      "2025-01-23           13.7980\n",
      "\n",
      "--- LSTM MODEL WARNINGS ---\n",
      "1. LSTM results are highly sensitive to lookback (10), units (50), epochs, batch size, and other hyperparameters.\n",
      "2. The model was trained on a relatively small dataset for LSTMs; overfitting is a risk.\n",
      "3. Accuracy heavily depends on the Stage 1 prediction of ALL exogenous variables.\n",
      "4. No extensive hyperparameter tuning or rigorous backtesting was performed.\n",
      "5. Evaluate these results critically.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "import tensorflow as tf # Use Tensorflow Keras\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm # Or from tqdm import tqdm for standard scripts\n",
    "\n",
    "# Make sure to install necessary libraries:\n",
    "# pip install tensorflow pandas numpy scikit-learn tqdm pandas_market_calendars\n",
    "try:\n",
    "    import pandas_market_calendars as mcal\n",
    "except ImportError:\n",
    "    print(\"Error: Please install pandas_market_calendars\")\n",
    "    print(\"Example: pip install pandas_market_calendars\")\n",
    "    exit()\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.get_logger().setLevel(logging.ERROR) # Suppress TensorFlow logs\n",
    "\n",
    "# --- Configuration ---\n",
    "N_STEPS_IN = 10       # Lookback window size (business days) - Hyperparameter\n",
    "N_FORECAST_BDAYS = 15 # Number of business days to predict\n",
    "EPOCHS_EXOG = 50      # Epochs for Stage 1 LSTM (adjust based on convergence)\n",
    "BATCH_SIZE_EXOG = 32\n",
    "EPOCHS_TARGET = 50    # Epochs for Stage 2 LSTM (adjust based on convergence)\n",
    "BATCH_SIZE_TARGET = 32\n",
    "LSTM_UNITS = 50       # Number of units in LSTM layers - Hyperparameter\n",
    "DROPOUT_RATE = 0.2    # Dropout rate for regularization\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    df_orig = pd.read_csv(\"UTI_Gold_ETF_MF_USD_01012023_31122024_ALL.csv\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: UTI_Gold_ETF_MF_USD_01012023_31122024_ALL.csv not found.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Preprocessing ---\n",
    "df = df_orig.copy()\n",
    "try:\n",
    "    df['All_Date'] = pd.to_datetime(df['All_Date'], format='%m-%d-%Y')\n",
    "    df.sort_values('All_Date', inplace=True)\n",
    "\n",
    "    # Define cleaning functions (same as before)\n",
    "    def parse_volume(value):\n",
    "        if isinstance(value, (int, float)): return value\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(',', '').strip().upper()\n",
    "            if 'K' in value: return float(value.replace('K', '')) * 1000\n",
    "            if 'M' in value: return float(value.replace('M', '')) * 1000000\n",
    "        try: return float(value)\n",
    "        except (ValueError, TypeError): return np.nan\n",
    "    def parse_percentage(value):\n",
    "        if isinstance(value, (int, float)): return value\n",
    "        if isinstance(value, str): value = value.replace('%', '').strip()\n",
    "        try: return float(value)\n",
    "        except (ValueError, TypeError): return np.nan\n",
    "\n",
    "    cols_to_clean = {\n",
    "        'ETF_Vol.': parse_volume, 'ETF_Change %': parse_percentage, 'USD_Change %': parse_percentage\n",
    "    }\n",
    "    for col, func in cols_to_clean.items():\n",
    "        if col in df.columns: df[col] = df[col].apply(func)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col != 'All_Date': df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Set index and resample to Business Days *before* scaling/sequencing\n",
    "    df_proc = df.set_index('All_Date').copy()\n",
    "    df_proc = df_proc.resample('B').last() # Use business day frequency\n",
    "    print(f\"Resampled data to business days. Shape before NaN handling: {df_proc.shape}\")\n",
    "\n",
    "    # Handle NaNs after resampling\n",
    "    cols_to_fill = df_proc.columns\n",
    "    df_proc[cols_to_fill] = df_proc[cols_to_fill].ffill()\n",
    "    if df_proc[cols_to_fill].isnull().sum().sum() > 0:\n",
    "        df_proc[cols_to_fill] = df_proc[cols_to_fill].bfill()\n",
    "\n",
    "    if df_proc.isnull().sum().sum() > 0:\n",
    "        print(\"Warning: NaNs remain after ffill/bfill. Dropping rows...\")\n",
    "        df_proc.dropna(inplace=True)\n",
    "        if df_proc.isnull().sum().sum() > 0: raise ValueError(\"NaNs persist after cleaning\")\n",
    "\n",
    "    print(f\"Shape after NaN handling: {df_proc.shape}\")\n",
    "    if len(df_proc) < N_STEPS_IN * 2: # Basic check for enough data\n",
    "         raise ValueError(f\"Insufficient data ({len(df_proc)} days) for lookback window ({N_STEPS_IN}) and training.\")\n",
    "\n",
    "    # Define target and exogenous columns\n",
    "    target_col = 'MF_NAV'\n",
    "    all_exog_cols = [col for col in df_proc.columns if col != target_col]\n",
    "    all_cols_ordered = [target_col] + all_exog_cols # Consistent order\n",
    "    df_proc = df_proc[all_cols_ordered] # Reorder dataframe columns\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during preprocessing: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 3. Scaling ---\n",
    "# Scale ALL columns together initially, then separate scalers if needed\n",
    "# Or scale column by column\n",
    "scalers = {}\n",
    "scaled_data = pd.DataFrame(index=df_proc.index)\n",
    "\n",
    "print(\"Scaling data...\")\n",
    "for col in df_proc.columns:\n",
    "    scalers[col] = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_col = scalers[col].fit_transform(df_proc[[col]])\n",
    "    scaled_data[col] = scaled_col.flatten()\n",
    "\n",
    "print(\"Data scaling complete.\")\n",
    "scaled_exog_data = scaled_data[all_exog_cols]\n",
    "scaled_target_data = scaled_data[[target_col]] # Keep as DataFrame\n",
    "\n",
    "# --- 4. Data Structuring for LSTM ---\n",
    "def create_sequences(input_data, n_steps_in, n_steps_out=1):\n",
    "    \"\"\"Creates sequences for LSTM.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(input_data) - n_steps_in - n_steps_out + 1):\n",
    "        seq_in = input_data[i : i + n_steps_in]\n",
    "        seq_out = input_data[i + n_steps_in : i + n_steps_in + n_steps_out]\n",
    "        X.append(seq_in)\n",
    "        y.append(seq_out)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- 5. Stage 1: Predict Future Exogenous Variables ---\n",
    "print(\"\\n--- Stage 1: Predicting Future Exogenous Variables (Multivariate LSTM) ---\")\n",
    "n_exog_features = len(all_exog_cols)\n",
    "\n",
    "# Structure data for Exogenous prediction LSTM\n",
    "# Input: window of past exog variables; Output: next step's exog variables\n",
    "X_exog, y_exog = create_sequences(scaled_exog_data.values, N_STEPS_IN, n_steps_out=1)\n",
    "y_exog = y_exog.reshape(y_exog.shape[0], n_exog_features) # Reshape y for Dense layer output\n",
    "\n",
    "print(f\"Exogenous data shapes: X={X_exog.shape}, y={y_exog.shape}\")\n",
    "\n",
    "# Build Stage 1 LSTM Model\n",
    "model_exog = Sequential([\n",
    "    LSTM(LSTM_UNITS, activation='relu', input_shape=(N_STEPS_IN, n_exog_features), return_sequences=True),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    LSTM(LSTM_UNITS, activation='relu', return_sequences=False),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(n_exog_features) # Output layer for all exogenous features\n",
    "])\n",
    "model_exog.compile(optimizer='adam', loss='mse')\n",
    "# model_exog.summary() # Optional: print model structure\n",
    "\n",
    "# Train Stage 1 Model\n",
    "print(f\"Training Stage 1 LSTM for {EPOCHS_EXOG} epochs...\")\n",
    "# Consider adding validation_split=0.1 or 0.2 if data allows\n",
    "# Or use EarlyStopping callback\n",
    "history_exog = model_exog.fit(X_exog, y_exog, epochs=EPOCHS_EXOG, batch_size=BATCH_SIZE_EXOG, verbose=0) # Verbose=0 for less output\n",
    "print(\"Stage 1 Training complete.\")\n",
    "\n",
    "# --- Iterative Forecasting for Exogenous Variables ---\n",
    "print(f\"Generating {N_FORECAST_BDAYS} steps of future exogenous predictions...\")\n",
    "last_sequence_exog = scaled_exog_data.values[-N_STEPS_IN:] # Last known historical sequence (scaled)\n",
    "current_batch_exog = last_sequence_exog.reshape((1, N_STEPS_IN, n_exog_features))\n",
    "future_exog_preds_scaled = []\n",
    "\n",
    "for i in tqdm(range(N_FORECAST_BDAYS), desc=\"Exog Forecast Steps\"):\n",
    "    # Get prediction for the next step\n",
    "    current_pred_exog = model_exog.predict(current_batch_exog, verbose=0)[0]\n",
    "    future_exog_preds_scaled.append(current_pred_exog)\n",
    "    # Update the batch for the next prediction: remove first step, append prediction\n",
    "    new_batch_entry = current_pred_exog.reshape((1, 1, n_exog_features))\n",
    "    current_batch_exog = np.append(current_batch_exog[:, 1:, :], new_batch_entry, axis=1)\n",
    "\n",
    "future_exog_preds_scaled = np.array(future_exog_preds_scaled)\n",
    "\n",
    "# --- Inverse Transform Exogenous Predictions ---\n",
    "print(\"Inverse transforming exogenous predictions...\")\n",
    "future_exog_preds_inv = pd.DataFrame(index=range(N_FORECAST_BDAYS), columns=all_exog_cols)\n",
    "for i, col in enumerate(all_exog_cols):\n",
    "    col_preds_scaled = future_exog_preds_scaled[:, i].reshape(-1, 1)\n",
    "    future_exog_preds_inv[col] = scalers[col].inverse_transform(col_preds_scaled).flatten()\n",
    "\n",
    "# Get the business day dates for the forecast period\n",
    "last_hist_date = df_proc.index.max()\n",
    "forecast_start_date = last_hist_date + pd.Timedelta(days=1) # Start day after last known date\n",
    "# Estimate end date needed to capture enough business days (add buffer)\n",
    "estimated_end_date = forecast_start_date + pd.Timedelta(days=N_FORECAST_BDAYS + 7) # Add buffer for weekends/holidays\n",
    "forecast_bdates = get_business_days(forecast_start_date, estimated_end_date)\n",
    "forecast_bdates = forecast_bdates[:N_FORECAST_BDAYS] # Select the exact number needed\n",
    "\n",
    "if len(forecast_bdates) != N_FORECAST_BDAYS:\n",
    "     print(f\"Warning: Could only generate {len(forecast_bdates)} business days. Adjusting N_FORECAST_BDAYS.\")\n",
    "     N_FORECAST_BDAYS = len(forecast_bdates)\n",
    "     # Trim predictions if necessary (shouldn't be if loop ran N_FORECAST_BDAYS times)\n",
    "     future_exog_preds_inv = future_exog_preds_inv.iloc[:N_FORECAST_BDAYS]\n",
    "\n",
    "\n",
    "future_exog_predictions_df = future_exog_preds_inv.set_index(forecast_bdates)\n",
    "print(\"Future exogenous predictions generated and inverse transformed.\")\n",
    "\n",
    "\n",
    "# --- 6. Stage 2: Predict Future MF_NAV ---\n",
    "print(\"\\n--- Stage 2: Predicting Future MF_NAV (LSTM with Exog) ---\")\n",
    "n_features_target = 1 + n_exog_features # Target + All Exog\n",
    "\n",
    "# Structure data for MF_NAV prediction LSTM\n",
    "# Input: window of past MF_NAV + past Exog; Output: next step's MF_NAV\n",
    "X_target, y_target = create_sequences(scaled_data[all_cols_ordered].values, N_STEPS_IN, n_steps_out=1)\n",
    "\n",
    "# --- FIX APPLIED HERE ---\n",
    "# Correctly select the target feature (MF_NAV at index 0) for the single output step\n",
    "y_target = y_target[:, 0, 0].reshape(-1, 1)\n",
    "# ------------------------\n",
    "\n",
    "print(f\"Target data shapes after structuring: X={X_target.shape}, y={y_target.shape}\") # Verify shapes\n",
    "\n",
    "# Build Stage 2 LSTM Model\n",
    "model_target = Sequential([\n",
    "    LSTM(LSTM_UNITS, activation='relu', input_shape=(N_STEPS_IN, n_features_target), return_sequences=True),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    LSTM(LSTM_UNITS, activation='relu', return_sequences=False),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(1) # Single output neuron for MF_NAV\n",
    "])\n",
    "model_target.compile(optimizer='adam', loss='mse')\n",
    "# model_target.summary() # Optional\n",
    "\n",
    "# Train Stage 2 Model\n",
    "print(f\"Training Stage 2 LSTM for {EPOCHS_TARGET} epochs...\")\n",
    "# Add validation_split for better monitoring during training\n",
    "history_target = model_target.fit(X_target, y_target, epochs=EPOCHS_TARGET, batch_size=BATCH_SIZE_TARGET, validation_split=0.1, verbose=0) # Added validation_split\n",
    "print(\"Stage 2 Training complete.\")\n",
    "\n",
    "# ... (rest of the code for iterative forecasting and output remains the same) ...\n",
    "\n",
    "# --- Iterative Forecasting for MF_NAV ---\n",
    "print(f\"Generating {N_FORECAST_BDAYS} steps of future MF_NAV predictions...\")\n",
    "\n",
    "# Get the last historical sequence (including target and exog, scaled)\n",
    "last_sequence_target = scaled_data[all_cols_ordered].values[-N_STEPS_IN:]\n",
    "current_batch_target = last_sequence_target.reshape((1, N_STEPS_IN, n_features_target))\n",
    "future_target_preds_scaled = []\n",
    "\n",
    "# Get the SCALED future exogenous predictions (needed for input)\n",
    "future_exog_preds_scaled_array = scaled_data[all_exog_cols].iloc[-N_FORECAST_BDAYS:].values # Re-use from stage 1 prediction\n",
    "# Alternative: re-predict scaled exog if preferred\n",
    "# Convert the inverse-transformed predictions back to scaled for input\n",
    "scaled_future_exog_for_input = pd.DataFrame(index=future_exog_predictions_df.index)\n",
    "for col in all_exog_cols:\n",
    "     scaled_future_exog_for_input[col] = scalers[col].transform(future_exog_predictions_df[[col]]).flatten()\n",
    "\n",
    "\n",
    "for i in tqdm(range(N_FORECAST_BDAYS), desc=\"Target Forecast Steps\"):\n",
    "    # Predict the next MF_NAV step\n",
    "    current_pred_target_scaled = model_target.predict(current_batch_target, verbose=0)[0]\n",
    "    future_target_preds_scaled.append(current_pred_target_scaled)\n",
    "\n",
    "    # Prepare the next input sequence\n",
    "    # Get the predicted exogenous variables for the *next* time step (i+1)\n",
    "    # Need to handle index carefully if i goes out of bounds\n",
    "    if i < N_FORECAST_BDAYS -1:\n",
    "        next_exog_scaled = scaled_future_exog_for_input.iloc[i+1].values # Use the exog prediction for the step we are about to predict NAV for\n",
    "    else:\n",
    "        # For the very last prediction, we don't have exog for i+1, reuse last predicted exog\n",
    "        # Or ideally, predict one more exog step in Stage 1\n",
    "        next_exog_scaled = scaled_future_exog_for_input.iloc[i].values # Simple fallback\n",
    "\n",
    "\n",
    "    # Create the new entry for the sequence: [predicted_NAV, next_predicted_Exog1, next_predicted_Exog2,...]\n",
    "    new_sequence_entry = np.concatenate([current_pred_target_scaled, next_exog_scaled]).reshape((1, 1, n_features_target))\n",
    "\n",
    "    # Update the batch: remove first step, append new entry\n",
    "    current_batch_target = np.append(current_batch_target[:, 1:, :], new_sequence_entry, axis=1)\n",
    "\n",
    "\n",
    "future_target_preds_scaled = np.array(future_target_preds_scaled)\n",
    "\n",
    "# --- Inverse Transform Target Predictions ---\n",
    "print(\"Inverse transforming target (MF_NAV) predictions...\")\n",
    "mf_nav_final_forecast_inv = scalers[target_col].inverse_transform(future_target_preds_scaled)\n",
    "\n",
    "# --- 7. Display Final Forecast ---\n",
    "final_forecast_df = pd.DataFrame({\n",
    "    'Predicted_MF_NAV': mf_nav_final_forecast_inv.flatten()\n",
    "    }, index=forecast_bdates) # Use the calculated business day dates\n",
    "\n",
    "print(\"\\n--- FINAL Predicted MF_NAV using Multivariate LSTM Approach ---\")\n",
    "print(f\"--- Forecast for Business Days: {forecast_bdates.min().date()} to {forecast_bdates.max().date()} ---\")\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "print(final_forecast_df)\n",
    "\n",
    "print(\"\\n--- LSTM MODEL WARNINGS ---\")\n",
    "print(f\"1. LSTM results are highly sensitive to lookback ({N_STEPS_IN}), units ({LSTM_UNITS}), epochs, batch size, and other hyperparameters.\")\n",
    "print(\"2. The model was trained on a relatively small dataset for LSTMs; overfitting is a risk.\")\n",
    "print(\"3. Accuracy heavily depends on the Stage 1 prediction of ALL exogenous variables.\")\n",
    "print(\"4. No extensive hyperparameter tuning or rigorous backtesting was performed.\")\n",
    "print(\"5. Evaluate these results critically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff9cc331-a302-4a6a-9fcd-a552372b4041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502, 20, 17)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba181733-5efb-45a6-9c2a-41ac4e388201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8534, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21cf2090-2efe-4a0a-a356-c61954a6f07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Starting preprocessing...\n",
      "Resampled data to business days. Shape before NaN handling: (522, 7)\n",
      "Shape after NaN handling: (522, 7)\n",
      "Preprocessing complete.\n",
      "Scaling data...\n",
      "Data scaling complete.\n",
      "\n",
      "--- Stage 1: Predicting Future Selected Exogenous Variables (Multivariate LSTM) ---\n",
      "Exogenous data shapes: X=(512, 10, 6), y=(512, 6)\n",
      "Training Stage 1 LSTM for 60 epochs...\n",
      "Stage 1 Training complete.\n",
      "  Final Training Loss (Exog): 0.0107\n",
      "  Final Validation Loss (Exog): 0.0102\n",
      "Generating 15 steps of future exogenous predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432db437048a44cdbcbe994fd99cce2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exog Forecast Steps:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse transforming exogenous predictions...\n",
      "Warning: Could only generate 14 business days in range. Adjusting N_FORECAST_BDAYS.\n",
      "Future selected exogenous predictions generated.\n",
      "\n",
      "--- Stage 2: Predicting Future MF_NAV (LSTM with Exog) ---\n",
      "Target stage data shapes: X=(512, 10, 7), y=(512, 1)\n",
      "Training Stage 2 LSTM for 60 epochs...\n",
      "Stage 2 Training complete.\n",
      "  Final Training Loss (Target): 0.0033\n",
      "  Final Validation Loss (Target): 0.0027\n",
      "Generating 14 steps of future MF_NAV predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618012426fde4605a442881bbb4c8632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Target Forecast Steps:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse transforming target (MF_NAV) predictions...\n",
      "\n",
      "--- FINAL Predicted MF_NAV using Two-Stage LSTM ---\n",
      "--- (Exog: ETF_Price, ETF_Change %, ETF_Vol., USD_Price, USD_Change %, Gold_Volume) ---\n",
      "--- Forecast for Business Days: 2025-01-02 to 2025-01-23 ---\n",
      "            Predicted_MF_NAV\n",
      "2025-01-02           14.8549\n",
      "2025-01-03           14.8487\n",
      "2025-01-06           14.8502\n",
      "2025-01-07           14.8506\n",
      "2025-01-08           14.8491\n",
      "2025-01-10           14.8472\n",
      "2025-01-13           14.8421\n",
      "2025-01-14           14.8321\n",
      "2025-01-15           14.8174\n",
      "2025-01-16           14.8016\n",
      "2025-01-17           14.7824\n",
      "2025-01-21           14.7682\n",
      "2025-01-22           14.7524\n",
      "2025-01-23           14.7351\n",
      "\n",
      "--- LSTM MODEL WARNINGS ---\n",
      "1. LSTM results depend heavily on hyperparameters (lookback=10, units=64, epochs, etc.) and data scaling.\n",
      "2. Trained on a relatively small dataset; overfitting is possible.\n",
      "3. Accuracy relies on Stage 1 predictions for exogenous variables.\n",
      "4. No extensive hyperparameter tuning performed.\n",
      "5. Evaluate critically; consider backtesting and comparison with simpler models.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm # Or from tqdm import tqdm\n",
    "\n",
    "# Make sure to install necessary libraries:\n",
    "# pip install tensorflow pandas numpy scikit-learn tqdm pandas_market_calendars\n",
    "try:\n",
    "    import pandas_market_calendars as mcal\n",
    "except ImportError:\n",
    "    print(\"Error: Please install pandas_market_calendars\")\n",
    "    print(\"Example: pip install pandas_market_calendars\")\n",
    "    exit()\n",
    "\n",
    "# Suppress warnings and TensorFlow logs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "# --- Configuration ---\n",
    "SELECTED_EXOG_COLS = ['ETF_Price', 'ETF_Change %', 'ETF_Vol.', 'USD_Price','USD_Change %','Gold_Volume'] # As per clarification above\n",
    "TARGET_COL = 'MF_NAV'\n",
    "N_STEPS_IN = 10       # Lookback window (business days) - Hyperparameter\n",
    "N_FORECAST_BDAYS = 15 # Number of business days to predict\n",
    "EPOCHS_EXOG = 60      # More epochs might be needed, adjust based on loss plots\n",
    "BATCH_SIZE_EXOG = 32\n",
    "EPOCHS_TARGET = 60    # More epochs might be needed, adjust based on loss plots\n",
    "BATCH_SIZE_TARGET = 32\n",
    "LSTM_UNITS = 64       # Increased units slightly\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    df_orig = pd.read_csv(\"UTI_Gold_ETF_MF_USD_01012023_31122024_ALL.csv\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: UTI_Gold_ETF_MF_USD_01012023_31122024_ALL.csv not found.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Preprocessing ---\n",
    "df = df_orig.copy()\n",
    "try:\n",
    "    print(\"Starting preprocessing...\")\n",
    "    df['All_Date'] = pd.to_datetime(df['All_Date'], format='%m-%d-%Y')\n",
    "    df.sort_values('All_Date', inplace=True)\n",
    "\n",
    "    # Define cleaning functions\n",
    "    def parse_volume(value):\n",
    "        if isinstance(value, (int, float)): return value\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(',', '').strip().upper()\n",
    "            if 'K' in value: return float(value.replace('K', '')) * 1000\n",
    "            if 'M' in value: return float(value.replace('M', '')) * 1000000\n",
    "        try: return float(value)\n",
    "        except (ValueError, TypeError): return np.nan\n",
    "    def parse_percentage(value):\n",
    "        if isinstance(value, (int, float)): return value\n",
    "        if isinstance(value, str): value = value.replace('%', '').strip()\n",
    "        try: return float(value)\n",
    "        except (ValueError, TypeError): return np.nan\n",
    "\n",
    "    # Apply cleaning\n",
    "    cols_to_clean = {'ETF_Vol.': parse_volume, 'ETF_Change %': parse_percentage, 'USD_Change %': parse_percentage}\n",
    "    for col, func in cols_to_clean.items():\n",
    "        if col in df.columns: df[col] = df[col].apply(func)\n",
    "    # Ensure all columns used are numeric\n",
    "    all_cols_to_use = [TARGET_COL] + SELECTED_EXOG_COLS\n",
    "    for col in df.columns: # Check all original columns for conversion\n",
    "        if col != 'All_Date': df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Select only necessary columns BEFORE setting index and resampling\n",
    "    df_selected = df[['All_Date'] + all_cols_to_use].copy()\n",
    "\n",
    "    # Set index and resample to Business Days\n",
    "    df_proc = df_selected.set_index('All_Date').copy()\n",
    "    df_proc = df_proc.resample('B').last() # Use business day frequency\n",
    "    print(f\"Resampled data to business days. Shape before NaN handling: {df_proc.shape}\")\n",
    "\n",
    "    # Handle NaNs after resampling\n",
    "    df_proc = df_proc.ffill().bfill() # Chain ffill and bfill\n",
    "    if df_proc.isnull().sum().sum() > 0:\n",
    "        print(f\"NaNs remaining after fill:\\n{df_proc.isnull().sum()}\")\n",
    "        df_proc.dropna(inplace=True)\n",
    "        if df_proc.isnull().sum().sum() > 0: raise ValueError(\"NaNs persist after cleaning\")\n",
    "\n",
    "    print(f\"Shape after NaN handling: {df_proc.shape}\")\n",
    "    if len(df_proc) < N_STEPS_IN * 2:\n",
    "         raise ValueError(f\"Insufficient data ({len(df_proc)} days) for lookback ({N_STEPS_IN})\")\n",
    "\n",
    "    # Ensure correct column order\n",
    "    df_proc = df_proc[[TARGET_COL] + SELECTED_EXOG_COLS]\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during preprocessing: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Scaling ---\n",
    "print(\"Scaling data...\")\n",
    "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_exog = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale target and exogenous features separately\n",
    "scaled_target = scaler_target.fit_transform(df_proc[[TARGET_COL]])\n",
    "scaled_exog = scaler_exog.fit_transform(df_proc[SELECTED_EXOG_COLS])\n",
    "\n",
    "# Combine scaled data for sequence creation in Stage 2\n",
    "scaled_data_combined = np.concatenate((scaled_target, scaled_exog), axis=1)\n",
    "print(\"Data scaling complete.\")\n",
    "\n",
    "# --- 4. Data Structuring for LSTM ---\n",
    "def create_sequences(input_data, n_steps_in, n_steps_out=1):\n",
    "    \"\"\"Creates sequences for LSTM.\"\"\"\n",
    "    X, y = [], []\n",
    "    n_samples = len(input_data)\n",
    "    for i in range(n_samples):\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        if out_end_ix > n_samples:\n",
    "            break\n",
    "        seq_x = input_data[i:end_ix, :]\n",
    "        seq_y = input_data[end_ix:out_end_ix, :] # Output includes all features for the step(s)\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- 5. Stage 1: Predict Future SELECTED Exogenous Variables ---\n",
    "print(\"\\n--- Stage 1: Predicting Future Selected Exogenous Variables (Multivariate LSTM) ---\")\n",
    "n_selected_exog_features = len(SELECTED_EXOG_COLS)\n",
    "\n",
    "# Structure data for Exogenous prediction LSTM (using only exog data)\n",
    "X_exog, y_exog_full = create_sequences(scaled_exog, N_STEPS_IN, n_steps_out=1)\n",
    "# Output should be the next step's values for the selected exog features\n",
    "y_exog = y_exog_full[:, 0, :] # Shape: (n_samples, n_selected_exog_features)\n",
    "\n",
    "print(f\"Exogenous data shapes: X={X_exog.shape}, y={y_exog.shape}\")\n",
    "\n",
    "# Build Stage 1 LSTM Model\n",
    "model_exog = Sequential([\n",
    "    LSTM(LSTM_UNITS, activation='relu', input_shape=(N_STEPS_IN, n_selected_exog_features), return_sequences=True),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    LSTM(LSTM_UNITS, activation='relu', return_sequences=False),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(n_selected_exog_features)\n",
    "])\n",
    "model_exog.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train Stage 1 Model\n",
    "print(f\"Training Stage 1 LSTM for {EPOCHS_EXOG} epochs...\")\n",
    "history_exog = model_exog.fit(X_exog, y_exog, epochs=EPOCHS_EXOG, batch_size=BATCH_SIZE_EXOG, validation_split=0.1, verbose=0, shuffle=False)\n",
    "print(\"Stage 1 Training complete.\")\n",
    "print(f\"  Final Training Loss (Exog): {history_exog.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Final Validation Loss (Exog): {history_exog.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Iterative Forecasting for Exogenous Variables ---\n",
    "print(f\"Generating {N_FORECAST_BDAYS} steps of future exogenous predictions...\")\n",
    "last_sequence_exog_scaled = scaled_exog[-N_STEPS_IN:]\n",
    "current_batch_exog = last_sequence_exog_scaled.reshape((1, N_STEPS_IN, n_selected_exog_features))\n",
    "future_exog_preds_scaled_list = []\n",
    "\n",
    "for i in tqdm(range(N_FORECAST_BDAYS), desc=\"Exog Forecast Steps\"):\n",
    "    current_pred_exog_scaled = model_exog.predict(current_batch_exog, verbose=0)[0]\n",
    "    future_exog_preds_scaled_list.append(current_pred_exog_scaled)\n",
    "    new_batch_entry = current_pred_exog_scaled.reshape((1, 1, n_selected_exog_features))\n",
    "    current_batch_exog = np.append(current_batch_exog[:, 1:, :], new_batch_entry, axis=1)\n",
    "\n",
    "future_exog_preds_scaled = np.array(future_exog_preds_scaled_list)\n",
    "\n",
    "# --- Inverse Transform Exogenous Predictions ---\n",
    "print(\"Inverse transforming exogenous predictions...\")\n",
    "future_exog_preds_inv = scaler_exog.inverse_transform(future_exog_preds_scaled)\n",
    "\n",
    "# Get the business day dates for the forecast period\n",
    "last_hist_date = df_proc.index.max()\n",
    "forecast_start_date = last_hist_date + pd.Timedelta(days=1)\n",
    "estimated_end_date = forecast_start_date + pd.Timedelta(days=N_FORECAST_BDAYS + 7) # Buffer\n",
    "\n",
    "# -- Business Day Calculation Helper --\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "def get_business_days_func(start_date, end_date):\n",
    "    try:\n",
    "        schedule = nyse.schedule(start_date=start_date, end_date=end_date)\n",
    "        business_days = pd.to_datetime(schedule.index).normalize()\n",
    "        return business_days\n",
    "    except Exception as e_bdays:\n",
    "        print(f\"Warning: Error getting NYSE holidays ({e_bdays}). Falling back to standard Bdays.\")\n",
    "        return pd.bdate_range(start=start_date, end=end_date)\n",
    "\n",
    "forecast_bdates = get_business_days_func(forecast_start_date, estimated_end_date)\n",
    "forecast_bdates = forecast_bdates[:N_FORECAST_BDAYS] # Select exact number\n",
    "\n",
    "if len(forecast_bdates) != N_FORECAST_BDAYS:\n",
    "     print(f\"Warning: Could only generate {len(forecast_bdates)} business days in range. Adjusting N_FORECAST_BDAYS.\")\n",
    "     N_FORECAST_BDAYS = len(forecast_bdates)\n",
    "     future_exog_preds_inv = future_exog_preds_inv[:N_FORECAST_BDAYS] # Trim if needed\n",
    "     future_exog_preds_scaled = future_exog_preds_scaled[:N_FORECAST_BDAYS] # Trim scaled version too\n",
    "\n",
    "\n",
    "future_exog_predictions_df = pd.DataFrame(future_exog_preds_inv, index=forecast_bdates, columns=SELECTED_EXOG_COLS)\n",
    "future_exog_preds_scaled_df = pd.DataFrame(future_exog_preds_scaled, index=forecast_bdates, columns=SELECTED_EXOG_COLS) # Keep scaled version\n",
    "\n",
    "print(\"Future selected exogenous predictions generated.\")\n",
    "\n",
    "\n",
    "# --- 6. Stage 2: Predict Future MF_NAV ---\n",
    "print(\"\\n--- Stage 2: Predicting Future MF_NAV (LSTM with Exog) ---\")\n",
    "n_features_target_stage = 1 + n_selected_exog_features # Target + Selected Exog\n",
    "\n",
    "# Structure data for MF_NAV prediction LSTM (using combined scaled data)\n",
    "X_target, y_target_full = create_sequences(scaled_data_combined, N_STEPS_IN, n_steps_out=1)\n",
    "# Output is only the target column (index 0 of combined data)\n",
    "y_target = y_target_full[:, 0, 0].reshape(-1, 1) # Shape: (n_samples, 1)\n",
    "\n",
    "print(f\"Target stage data shapes: X={X_target.shape}, y={y_target.shape}\")\n",
    "\n",
    "# Build Stage 2 LSTM Model\n",
    "model_target = Sequential([\n",
    "    LSTM(LSTM_UNITS, activation='relu', input_shape=(N_STEPS_IN, n_features_target_stage), return_sequences=True),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    LSTM(LSTM_UNITS, activation='relu', return_sequences=False),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(1) # Single output neuron for MF_NAV\n",
    "])\n",
    "model_target.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train Stage 2 Model\n",
    "print(f\"Training Stage 2 LSTM for {EPOCHS_TARGET} epochs...\")\n",
    "history_target = model_target.fit(X_target, y_target, epochs=EPOCHS_TARGET, batch_size=BATCH_SIZE_TARGET, validation_split=0.1, verbose=0, shuffle=False)\n",
    "print(\"Stage 2 Training complete.\")\n",
    "print(f\"  Final Training Loss (Target): {history_target.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Final Validation Loss (Target): {history_target.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "# --- Iterative Forecasting for MF_NAV ---\n",
    "print(f\"Generating {N_FORECAST_BDAYS} steps of future MF_NAV predictions...\")\n",
    "\n",
    "# Get the last historical sequence (scaled target + scaled exog)\n",
    "last_sequence_target_stage = scaled_data_combined[-N_STEPS_IN:]\n",
    "current_batch_target_stage = last_sequence_target_stage.reshape((1, N_STEPS_IN, n_features_target_stage))\n",
    "future_target_preds_scaled_list = []\n",
    "\n",
    "for i in tqdm(range(N_FORECAST_BDAYS), desc=\"Target Forecast Steps\"):\n",
    "    # Predict the next MF_NAV step (scaled)\n",
    "    current_pred_target_scaled = model_target.predict(current_batch_target_stage, verbose=0)[0]\n",
    "    future_target_preds_scaled_list.append(current_pred_target_scaled)\n",
    "\n",
    "    # Prepare the next input sequence if not the last step\n",
    "    if i < N_FORECAST_BDAYS - 1:\n",
    "        # Get the *predicted* scaled exogenous variables for the *next* time step\n",
    "        next_exog_scaled = future_exog_preds_scaled_df.iloc[i].values # Note: index i corresponds to the prediction for step i+1 relative to start\n",
    "\n",
    "        # Create the new entry for the sequence: [predicted_NAV, next_predicted_Exog1, ...]\n",
    "        new_sequence_entry = np.concatenate([current_pred_target_scaled, next_exog_scaled]).reshape((1, 1, n_features_target_stage))\n",
    "\n",
    "        # Update the batch: remove first step, append new entry\n",
    "        current_batch_target_stage = np.append(current_batch_target_stage[:, 1:, :], new_sequence_entry, axis=1)\n",
    "\n",
    "\n",
    "future_target_preds_scaled = np.array(future_target_preds_scaled_list)\n",
    "\n",
    "# --- Inverse Transform Target Predictions ---\n",
    "print(\"Inverse transforming target (MF_NAV) predictions...\")\n",
    "mf_nav_final_forecast_inv = scaler_target.inverse_transform(future_target_preds_scaled)\n",
    "\n",
    "# --- 7. Display Final Forecast ---\n",
    "final_forecast_df = pd.DataFrame({\n",
    "    'Predicted_MF_NAV': mf_nav_final_forecast_inv.flatten()\n",
    "    }, index=forecast_bdates)\n",
    "\n",
    "print(\"\\n--- FINAL Predicted MF_NAV using Two-Stage LSTM ---\")\n",
    "print(f\"--- (Exog: {', '.join(SELECTED_EXOG_COLS)}) ---\")\n",
    "print(f\"--- Forecast for Business Days: {forecast_bdates.min().date()} to {forecast_bdates.max().date()} ---\")\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "print(final_forecast_df)\n",
    "\n",
    "print(\"\\n--- LSTM MODEL WARNINGS ---\")\n",
    "print(f\"1. LSTM results depend heavily on hyperparameters (lookback={N_STEPS_IN}, units={LSTM_UNITS}, epochs, etc.) and data scaling.\")\n",
    "print(\"2. Trained on a relatively small dataset; overfitting is possible.\")\n",
    "print(\"3. Accuracy relies on Stage 1 predictions for exogenous variables.\")\n",
    "print(\"4. No extensive hyperparameter tuning performed.\")\n",
    "print(\"5. Evaluate critically; consider backtesting and comparison with simpler models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e719a-d838-4fc1-9da9-5b0689997099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
