{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "HvDOg5A1P3qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Pytorch provides 2 data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to pre-loaded datasets as well as your own data.\n",
        "- Dataset stores the samples and their corresponding labels and DataLoader wraps an iterable around the dataset to enable easy access to the samples."
      ],
      "metadata": {
        "id": "SSNO2E0En5Ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading a Dataset**\n",
        "\n",
        "Here is an example of how to load the Fashion-MNIST dataset from TorchVision. We load FashionMNIST Datset with the following parameters:\n",
        "- root is the path where tain/test data is stored.\n",
        "- train specifies training or test dataset.\n",
        "- download=True downloads the data from the internet if it's not available at root\n",
        "- transform and target_transform specify the feature and label transformations."
      ],
      "metadata": {
        "id": "z0OffJ2BOwoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD2axoxRnvqa",
        "outputId": "bc185e54-6b7b-4fbc-ff60-9bcf88b51869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.7MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 271kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.05MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 10.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor,Lambda\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_data=datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data=datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transforms**\n",
        "\n",
        "- We use Transforms to perform some manipulation of the data and make it suitable for training\n",
        "- All TorchVision datasets have 2 parameters - transform to modify the features and target_transform to modify the labels.\n",
        "\n",
        "The FashionMNIST features are PIL Image  format, and labels are integers. For training,we need the features as normalized tensors and the labels as one hot encoded tensors. To make these transformations we use ToTensor and Lambda."
      ],
      "metadata": {
        "id": "sRYMTBQ-Q1hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data transforms\n",
        "from torchvision.transforms import ToTensor,Lambda\n",
        "\n",
        "ds=datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y:torch.zeros(10,dtype=torch.float).scatter_(0,torch.tensor(y),value=1))\n",
        ")"
      ],
      "metadata": {
        "id": "r9AQQ82jQwSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- toTensor() ToTensor converts a PIL Image or Numpy ndarray into a FloatTensor and scales the image pixels intensity values in the range [0.,1.]\n",
        "- Lambda Transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor."
      ],
      "metadata": {
        "id": "bD_XMrGLSgp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#operations on Tensors\n",
        "shape=(2,3,)\n",
        "rand_tensor=torch.rand(shape)\n",
        "ones_tensor=torch.ones(shape)\n",
        "zeros_tensor=torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"One Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zero Tensor: \\n {zeros_tensor} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awCx8883SgZ9",
        "outputId": "f7c2e586-2931-4272-b6c6-6920da9c5ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.8701, 0.8045, 0.7648],\n",
            "        [0.9054, 0.0571, 0.5923]]) \n",
            "\n",
            "One Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zero Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor=torch.rand(3,4)\n",
        "print(f\"Shape: \\n {tensor.shape} \\n\")\n",
        "print(f\"Datatype: \\n {tensor.dtype} \\n\")\n",
        "print(f\"Device Tensor is stored in: \\n {tensor.device} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8PjA8qjSf4X",
        "outputId": "6a57b92d-76ca-4062-8c2a-b192122382c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: \n",
            " torch.Size([3, 4]) \n",
            "\n",
            "Datatype: \n",
            " torch.float32 \n",
            "\n",
            "Device Tensor is stored in: \n",
            " cpu \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tensor to NumPy array\n",
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0X7sVdkUpsE",
        "outputId": "bab6335a-2450-47c2-f922-cc0a71db4c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Numpy to Tensor\n",
        "t=torch.from_numpy(n)\n",
        "print(f\"t: {t}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9PK6ZUmU4sz",
        "outputId": "25c72370-3eba-473e-ea8a-5f5fa0bc2d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xW6WE56xVQQz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}